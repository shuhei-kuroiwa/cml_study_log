# cml_study_log
A learning record of econometrics and causal machine learning.

2/28: today i reviwed mu sand_01 code and just learned analyzing dataset with deadly null is waste of time. i dont know why but i used DML and GRF for this GIGO data, probably i just wanted to use these cool method. after that, i started to review sand_02 and consider df_100 is valid or not. i read sueishi text and learned Heckman method and Inverse Mills Ratio. however it was too hard to understand, just got only in words(not in math). later, i kept considering which model is reasonable to explain my question "which variable is effective for games' sale", first of all peak_ccu is just a moment of enthusiasm and median_playtime or total_num_reviews should be better however these variables have different meaning. median_playtime tend to represent sales' quality and total_num_reviews tend to represent sales' quantity. so using both and comparing should be reasonable. but total_num_reviews could be also represent backlash on the internet, so pct_pos_recent or positive could be used. and based on that, for now i wanna check CATE of discount with these variavles. and i expect this could have some problems, first it could hold simultaneous issue, second, low-end game problem. In the first place, if no discounts are even set and the service is left unattended, this could mean 'discount is pointless', thus i still wanna cut data, but this heckman-dml method is truly hard to understand math. so i will use text.

2/27: following yesterday, i am checking my code of Sand_01. in this Sandbox, at first i didnt find this positive column is all null and thi OLS gave me crazy scores(R**2 = 0, and Kurtosis = 51455.440), so this could be valid reason to change analysis dataset and use log transformation. and i felt standardized data formatting is quit important bcz for instance, if i dont use lower and repeted 'Peak_CCU' in my code, it would be too hard to fix later and also hard for colleagues(in the future). thus i will need to get data formatting skill(but not that urgent).
2/26: i started carefull consideration about my analysis, to find where i cant understand and what part of textbook i need to read. at first, df_100 is dataset that only includes ccu more than 100, it should have some bias probably sueishi textbook' 6th chapter explains. 2nd, why i chose Peak CCU as dependant variavle? it is just one of indexes, thus it is unatural to not try other variavles, for example median_playtime_forever, estimated_owners, and so on. so i think i need to analyze others but i dont know how to decide which is better variable.. moreover, i think checking code is good way to study, my codes (written by Gemini) should be coneccted to formula on my textbook, and if i can get understood these codes and get a skill to write by myself, it would be a big advance of my math and coding. i made new samdvox series and am going to crash my models on my sand_2.
2/25: i started to prepare for my seminar presentation, in this presentation, i need to decide what i am going to analyize. actually, i expect i have already done my analysis part and need to understand how these methods work. therefore, i returned to Yabu textbook, and review endogenity and simultanius parts. i just wanna keep doing this way.
2/24: i used 2024 dataset and tried to use as lag_variable, for solving simultanious issue. as a method, i used 2SLS and IV-DML, because normal 2SLS can only explain linear relationship, thus i implemented both of them and compared. normal 2sls's β was 0.1760, and iv_dml β was 0.3493, so optimistically, i can predict this is effective even without endogeneity, but realisticly, serial correlation could be happened. however, for now i wanna go back text and codes' deep understanding because im using gemini and cant write these code and i feel i need to get understand background mathmatics.  
2/23:today i tried 2024's data as Lagged Independent Variable. to check this new variable is effective, i used Lasso but this term's positive number was somehow proved to be the best. i"m gonna try debugging.
2/22:today i used Random forest regressor to get to know which variable predicts CCU and mediun play time best, as a result, log_positive is the best one to predict and omit posi_nega_ratio, and finally i used dml to predict how actually log_positive and log_negative work, it was lower than i expected. and i noticed that it could have Simultaneous equation, so used Lasso to identify IV, but best IV was log_positive itself. thus, i installed 2024's steam dataset and probably tmr use as IV. at the end, i found that i should write README frequently, cuz it is too annoying to write it at one time.
2/21:today following yesterday’s OLS, i fixed log positive and log negative and just compared models with other variables, such as positive negative ratio, play time medium and so on. however as a result, probably because of multicollinearity, log positive turned negative, and ply time truly didn’t seem to have effect to CCU. moreover, positive and CCU should be simultaneous equation, thus i need to consider precise IV. in addition, i don’t know hot FE and other variables could be compatible. an first of all,  manipulating variables is too hard , so i wanna go back to textbook and gonna think instrumental validity. in conclusion, i will keep analyzing steadily. to be honest, i feel like to just estimate like kaggle competition, without strict theory.  
2/21:今日は、昨日のOLSに続いて好評価数と低評価数の対数を固定したまま他の変数(高評価低評価比率やプレイ時間の中央値など)を組み合わせて比較してみたが、結果として比率を入れるとおそらく多重共線性で好評価数の係数がマイナスに触れてしまい、また、プレイ時間は全くCCUに影響を持たないという結果になった。また、そもそもの高評価数とCCUは同時方程式感があり、IVを検討しないといけないと思っている。また、固定効果に関しても他の変数との併用の検討や、そもそものモデルに対する変数の足し引きが難しく、教科書と並行しながら操作の妥当性を考えたい。またとりあえず色々試しながら地道に分析を進めていきたい。正直一旦理論の厳密さはおいて、Kaggle的に予測をしてみたい気もする。  
2/20:今日はゲームのPublisherについて固定効果モデルを回してみた。薮本で学んだ内容だが、仕組みなどをかなり忘れていたので復習した。薮本では「時間を通じて一定の変数は固定効果モデルをかけると変数が消えてしまうからだめ」と学んだのだが、Geminiによると実際はそれでも使う場合が多いらしい、理由はまだ理解できていない。また、サンプル数が1の場合だとその個体の定数が極端に説明力を持ってしまうためDropすることも検討した方がいいが、実証分析ではなく予測にフォーカスする場合はそれも重要な説明力だと判断するため残すこともあるらしい。今回は売れたゲームの売れた要因を知ることが目的なので、残さずにn＝1のものは省くことにする。また、高評価数と低評価数の対数と価格に対してハウスマン検定を行った。固定効果を用いることが妥当となったが、実は概念理解しかしておらず、数理的な理解はできてないので詰めたい。  
7日目(2月20日):もともと10日間のみ触るリポジトリのつもりだったが、作業ログが残っていくのが思っていたより楽しかったので、長期的に記録できるように10days_CausalML_Bootcampからcml_study_logに名前を変更した。  
変更点→リポジトリ名とdescriptionおよび作業ログを日にちが上に積まれていくように修正した。また、〜日目と書かずに今日の日付を書くことで続けやすいように変更した。元のままだと毎日やらないといけないっていう強迫観念に駆られてしんどい  
6日目(2月19日):高評価数（Positive）の値が一つも入っていないことに気づき、CCUの分析にはかなり重要な値だと判断したので別の主要説明変数を欠損していないデータセットを使うことにした。peak_ccuの閾値を調整した結果、100以上で分けた時はデータ数が2000程度に残りかつlod_ccuと元変数の相関も0.37程度と妥協できる範囲だったので、このdfを使うことに決めた。高評価数と低評価数をログ化してOLSモデルをいくつか回してみて比較しようとしたが、統計量の解釈が思っていたより難しくまだパッとしない。  
5日目(2月18日):線形モデルのGMMとその性質を導出したが、それより先が難しい上にとてもつまらなく、一旦データ分析の実装を並行して行うことにした。Kaggkeから持ってきたSteamデータセットをいじっていくことにしたのだが、データのカラム整理や最低限のクレンジングをしても、CCUの冪乗分布が酷すぎてまともに分析が行えなかった。そこでHeckmanの二段階推定ぽいもの（CCUが0のものと1以上、あるいは100未満と100以上）をやろうとしたが、どちらにせよ外れ値が支配的すぎる状態は変わらないので、Heatmapの結果もそこまで変わらなかった。明日は対数化など行ったり閾値をもう少し調整するか、末石本の続きをやりつつ6章のHeckman推定まで進めてみるかのどちらかをやりたい。  
4日目(2月17日):ATE・無視可能性の仮定・LATEの導出を行った。GMMを少し見て、ATEとLATEの復習が優先と感じ3章に戻った。台湾の春節のためイレギュラーが多く生じ想定より進めることができなかったが、十分に必要な概念の定着に勤しむことができたと感じる。明日からはGMMの章に戻る。 
3日目(2月16日):TSLSEの性質・J統計量・モーメント法・一般のモーメント法の導出を行った。末石本の4章の内容は終了し、GMMの直前まで行ったところで論文を読んだところ全く数理が繋がらなかったので、[4章（行列表記と漸近理論）→5章（GMM）→3章（ATEとLATE）→論文（導出・理解(CATE・ネイマン直交・SとかTラーナー・DML)）→論文（実装）]　以上のロードマップを仮定して明日からGMMの数理導出と実践のための理解に時間をかけていく。論文を読んでサクッとライブラリで回すなどもやりたいが、正直全く実装の際にどのように数理が動いているかのイメージが湧かず、数理的基礎がまだ不足していると感じる。  
2日目(2月15日):Wald統計量とF統計量の違い・GLS（スカラーと行列）・FGLS・TSLSの導出を行なった。昨日と比べてかけた時間の割に進められたページ自体はやや少なかったが、内容そのものにはかなり理解に時間を要した。Gitに上げ直すフェーズで、昨日と比べて導出に使ったページが多いことに驚いた。行列計算の細部、デルタ法やMinkowskiの不等式などはかなり妥協しながら進めている部分があり、将来的には回収したい。実装に関して、明日か明後日にはGMMに入ると思うので、そのくらいのタイミングで並行して再開していこうと思う。  
1日目(2月14日):OLSE・不偏性・一致性・漸近正規性・WALD検定（末石本54〜62ページ）の自力導出および証明を行った。Wald統計量で躓いていたが、写経してみると論理が追えた。Steamデータに関して、データの最低限の整形（欠損値除去、入れ違いとなっていたカラムの整理など）を行った。Chen（2025）の内容を何らかの形で実装したかったが、GMMの深い理解が必須なように感じたので、末石本の5章途中まで行うことを平行しながら、実装内容の数理を詰めようと思っている。明日はGLS推定量の導出や、いったん厳密な理解を後回しにDMLの実装などを試みたい。  
0日目(2月13日):Chen（2025）の論文を理解するために、行き詰まっている末石本（計量経済学）の4〜5章の自力導出から、Steamデータを用いた因果MLの実装までを10日(2月14日〜2月23日)で完遂するための、記録リポジトリです（←長期リポジトリに修正）。リポジトリを作成し個別のファイルを作成した。
